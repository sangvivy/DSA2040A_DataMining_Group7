{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "995f2b21",
   "metadata": {},
   "source": [
    "# ETL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b03e424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "905ec62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CSV files:\n",
      "  ..\\data\\raw\\postings.csv\n",
      "  ..\\data\\raw\\companies\\companies.csv\n",
      "  ..\\data\\raw\\companies\\company_industries.csv\n",
      "  ..\\data\\raw\\companies\\company_specialities.csv\n",
      "  ..\\data\\raw\\companies\\employee_counts.csv\n",
      "  ..\\data\\raw\\jobs\\benefits.csv\n",
      "  ..\\data\\raw\\jobs\\job_industries.csv\n",
      "  ..\\data\\raw\\jobs\\job_skills.csv\n",
      "  ..\\data\\raw\\jobs\\salaries.csv\n",
      "  ..\\data\\raw\\mappings\\industries.csv\n",
      "  ..\\data\\raw\\mappings\\skills.csv\n",
      "  ..\\data\\transformed\\companies\\it_companies.csv\n",
      "  ..\\data\\transformed\\companies\\it_company_industries.csv\n",
      "  ..\\data\\transformed\\companies\\it_employee_counts.csv\n",
      "  ..\\data\\transformed\\companies\\it_specialities.csv\n",
      "  ..\\data\\transformed\\jobs\\it_benefits.csv\n",
      "  ..\\data\\transformed\\jobs\\it_job_industries_cleaned.csv\n",
      "  ..\\data\\transformed\\jobs\\it_job_skills.csv\n",
      "  ..\\data\\transformed\\jobs\\it_salaries.csv\n",
      "  ..\\data\\transformed\\mappings\\industries_it_only.csv\n",
      "  ..\\data\\transformed\\postings\\postings_it_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "base = Path(\"..\")  \n",
    "csv_files = list(base.rglob(\"*.csv\"))\n",
    "\n",
    "print(\"Found CSV files:\")\n",
    "for file in csv_files:\n",
    "    print(f\"  {file}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d23c071",
   "metadata": {},
   "source": [
    "#### 1. Filter IT Companies from `company_industries.csv`\n",
    "Extracting only companies in the \"IT Services and IT Consulting\" industry to get relevant `company_id`s for our analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a8f8091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['company_id', 'industry'], dtype='object')\n",
      "Filtered data saved to: ../data/transformed/companies/it_company_industries.csv\n",
      "Remaining rows: 2130\n"
     ]
    }
   ],
   "source": [
    "input_path = \"../data/raw/companies/company_industries.csv\"\n",
    "output_path = \"../data/transformed/companies/it_company_industries.csv\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Check available columns\n",
    "print(df.columns)\n",
    "\n",
    "# Filter for IT Services and IT Consulting\n",
    "it_df = df[df['industry'] == 'IT Services and IT Consulting']\n",
    "\n",
    "# Save filtered data\n",
    "it_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Filtered data saved to: {output_path}\")\n",
    "print(\"Remaining rows:\", it_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42e26a1",
   "metadata": {},
   "source": [
    "#### 2. Filter related company data\n",
    "Using `company_id`s from the filtered IT companies to extract relevant records from:\n",
    "- `raw/companies/companies.csv`\n",
    "- `raw/companies/company_specialities.csv`\n",
    "- `raw/companies/employee_counts.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68035f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "companies filtered → 2130 rows saved to ../data/transformed/companies/it_companies.csv\n",
      "specialities filtered → 19623 rows saved to ../data/transformed/companies/it_specialities.csv\n",
      "employee_counts filtered → 3345 rows saved to ../data/transformed/companies/it_employee_counts.csv\n"
     ]
    }
   ],
   "source": [
    "# Load company_ids of IT companies\n",
    "it_ids = pd.read_csv(\"../data/transformed/companies/it_company_industries.csv\")[\"company_id\"]\n",
    "\n",
    "# Define input/output paths\n",
    "input_files = {\n",
    "    \"companies\": \"../data/raw/companies/companies.csv\",\n",
    "    \"specialities\": \"../data/raw/companies/company_specialities.csv\",\n",
    "    \"employee_counts\": \"../data/raw/companies/employee_counts.csv\"\n",
    "}\n",
    "\n",
    "output_base = \"../data/transformed/companies\"\n",
    "\n",
    "# Filter and save each dataset\n",
    "for name, path in input_files.items():\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    # Filter rows based on company_id\n",
    "    filtered_df = df[df[\"company_id\"].isin(it_ids)]\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_base, exist_ok=True)\n",
    "    \n",
    "    # Save the filtered data\n",
    "    out_path = f\"{output_base}/it_{name}.csv\"\n",
    "    filtered_df.to_csv(out_path, index=False)\n",
    "    \n",
    "    print(f\"{name} filtered → {filtered_df.shape[0]} rows saved to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0717b04a",
   "metadata": {},
   "source": [
    "#### 3. Transform `time_recorded` in `it_employee_counts.csv`\n",
    "The `time_recorded` column contains Unix timestamps. We'll convert them into human-readable datetime format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cb6b3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 'time_recorded' converted to datetime.\n",
      "   company_id       time_recorded\n",
      "0      729238 2024-04-05 19:42:53\n",
      "1     2934678 2024-04-05 19:42:53\n",
      "2    15984730 2024-04-05 19:42:53\n",
      "3     6618000 2024-04-05 19:42:53\n",
      "4    75056372 2024-04-05 19:42:53\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Load filtered employee counts data\n",
    "emp_df = pd.read_csv(\"../data/transformed/companies/it_employee_counts.csv\")\n",
    "\n",
    "# Convert Unix timestamp to datetime\n",
    "emp_df[\"time_recorded\"] = pd.to_datetime(emp_df[\"time_recorded\"], unit=\"s\")\n",
    "\n",
    "# Save the updated DataFrame\n",
    "emp_df.to_csv(\"../data/transformed/companies/it_employee_counts.csv\", index=False)\n",
    "\n",
    "print(\" 'time_recorded' converted to datetime.\")\n",
    "print(emp_df[[\"company_id\", \"time_recorded\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3329bcb6",
   "metadata": {},
   "source": [
    "#### 4. Transforming Industries Dataset: Filtering for IT-Related Roles\n",
    "\n",
    "To identify job postings relevant to IT and tech, we'll apply a mapping filter to the `industries` field. This step is crucial because some industries lack proper names and are only identifiable by keywords. We'll use two sets of patterns:\n",
    "\n",
    "- **Inclusion patterns** (indicating relevance to IT)\n",
    "- **Exclusion patterns** (to avoid false positives from overlapping terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c98fbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered IT-related industries saved to: ../data/transformed/mappings/industries_it_only.csv\n"
     ]
    }
   ],
   "source": [
    "# Load raw industry mappings\n",
    "industry_path = \"../data/raw/mappings/industries.csv\"\n",
    "industries_df = pd.read_csv(industry_path)\n",
    "\n",
    "# Define IT-related inclusion and exclusion patterns\n",
    "it_patterns = [\n",
    "    r\"\\bIT\\b\",\n",
    "    r\"\\bInformation Technology\\b\",\n",
    "    r\"\\bSoftware\\b\",\n",
    "    r\"\\bDeveloper\\b\",\n",
    "    r\"\\bEngineer\\b\",\n",
    "    r\"\\bProgrammer\\b\",\n",
    "    r\"\\bTech\\b\",\n",
    "    r\"\\bData\\b\",\n",
    "    r\"\\bCloud\\b\",\n",
    "    r\"\\bSystem\\b\",\n",
    "    r\"\\bNetwork\\b\"\n",
    "]\n",
    "\n",
    "exclude_patterns = [\n",
    "    r\"\\bBuilding\\b\",\n",
    "    r\"\\bHVAC\\b\",\n",
    "    r\"\\bMaintenance\\b\",\n",
    "    r\"\\bFacilities?\\b\",\n",
    "    r\"\\bStaff\\b\",\n",
    "    r\"\\bConstruction\\b\",\n",
    "    r\"\\bMechanical\\b\",\n",
    "    r\"\\bElectrical\\b\",\n",
    "    r\"\\bCivil\\b\",\n",
    "    r\"\\bManufacturing\\b\",\n",
    "    r\"\\bProject Manager\\b\",\n",
    "    r\"\\bProject Engineer\\b\",\n",
    "    r\"\\bQuality\\b\",\n",
    "    r\"\\bSafety\\b\",\n",
    "    r\"\\bLogistics\\b\",\n",
    "    r\"\\bSupply Chain\\b\",\n",
    "    r\"\\bField\\b\",\n",
    "    r\"\\bService\\b\",\n",
    "    r\"\\bSupport\\b\", \n",
    "    r\"\\bStructural\\b\",\n",
    "    r\"\\bProcess\\b\", \n",
    "    r\"\\bDesign\\b\",\n",
    "    r\"\\bInfrastructure\\b\",\n",
    "    r\"\\bEnergy\\b\",\n",
    "]\n",
    "\n",
    "# Function to determine if an industry name is IT-related\n",
    "def is_it_industry(industry_name):\n",
    "    if not isinstance(industry_name, str):\n",
    "        return False\n",
    "    name = industry_name.lower()\n",
    "    include = any(re.search(pat, name, flags=re.IGNORECASE) for pat in it_patterns)\n",
    "    exclude = any(re.search(pat, name, flags=re.IGNORECASE) for pat in exclude_patterns)\n",
    "    return include and not exclude\n",
    "\n",
    "# Apply the filter\n",
    "industries_df[\"is_it\"] = industries_df[\"industry_name\"].apply(is_it_industry)\n",
    "it_industries = industries_df[industries_df[\"is_it\"]].drop(columns=[\"is_it\"])\n",
    "\n",
    "# Save transformed output\n",
    "output_path = \"../data/transformed/mappings/industries_it_only.csv\"\n",
    "it_industries.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Filtered IT-related industries saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a9bdc9",
   "metadata": {},
   "source": [
    "#### 5. Filter job IDs by IT-related industries\n",
    "\n",
    "Using the filtered `industry_id`s from the transformed industries dataset, we filter job IDs from `../data/raw/jobs/job_industries.csv` to retain only jobs belonging to IT-related industries.\n",
    "\n",
    "We then use these filtered `job_id`s to subset the following datasets:\n",
    "\n",
    "- `../data/raw/jobs/benefits.csv`\n",
    "- `../data/raw/jobs/job_skills.csv`\n",
    "- `../data/raw/jobs/salaries.csv`\n",
    "\n",
    "Only rows with `job_id`s present in the IT subset are retained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b1608e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "job_industries_path = Path(\"../data/raw/jobs/job_industries.csv\")\n",
    "benefits_path = Path(\"../data/raw/jobs/benefits.csv\")\n",
    "skills_path = Path(\"../data/raw/jobs/job_skills.csv\")\n",
    "salaries_path = Path(\"../data/raw/jobs/salaries.csv\")\n",
    "output_dir = Path(\"../data/transformed/jobs\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load job_industries.csv\n",
    "job_industries_df = pd.read_csv(job_industries_path)\n",
    "\n",
    "# Load the filtered IT-related industries\n",
    "it_industries_path = Path(\"../data/transformed/mappings/industries_it_only.csv\")\n",
    "it_industries_df = pd.read_csv(it_industries_path)\n",
    "\n",
    "# Filter job IDs based on IT-related industry IDs\n",
    "it_industry_ids = set(it_industries_df[\"industry_id\"])\n",
    "filtered_job_ids = job_industries_df[job_industries_df[\"industry_id\"].isin(it_industry_ids)][\"job_id\"].unique()\n",
    "\n",
    "# Save filtered job_industries.csv\n",
    "job_industries_df[job_industries_df[\"job_id\"].isin(filtered_job_ids)]\\\n",
    "    .to_csv(output_dir / \"it_job_industries.csv\", index=False)\n",
    "\n",
    "# Filter and save other job datasets\n",
    "pd.read_csv(benefits_path).query(\"job_id in @filtered_job_ids\")\\\n",
    "    .to_csv(output_dir / \"it_benefits.csv\", index=False)\n",
    "\n",
    "pd.read_csv(skills_path).query(\"job_id in @filtered_job_ids\")\\\n",
    "    .to_csv(output_dir / \"it_job_skills.csv\", index=False)\n",
    "\n",
    "pd.read_csv(salaries_path).query(\"job_id in @filtered_job_ids\")\\\n",
    "    .to_csv(output_dir / \"it_salaries.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f7da19",
   "metadata": {},
   "source": [
    "#### 6. Check if industry_id and job_id are linked correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aa6436c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       job_id  industry_id                  industry_name\n",
      "0  3884431567           96  IT Services and IT Consulting\n",
      "1  3884916106           96  IT Services and IT Consulting\n",
      "2  3884916106            4           Software Development\n",
      "4  3884431568           96  IT Services and IT Consulting\n",
      "5  3861704803           96  IT Services and IT Consulting\n",
      "Deleted old file: ../data/transformed/jobs/it_job_industries.csv\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "industries_df = pd.read_csv(\"../data/transformed/mappings/industries_it_only.csv\")\n",
    "job_industries_df = pd.read_csv(\"../data/transformed/jobs/it_job_industries.csv\")\n",
    "\n",
    "# Rename 'name' to 'industry_name' if needed\n",
    "if 'name' in industries_df.columns and 'industry_name' not in industries_df.columns:\n",
    "    industries_df = industries_df.rename(columns={\"name\": \"industry_name\"})\n",
    "\n",
    "# Merge to inspect job_id with industry info\n",
    "merged_df = job_industries_df.merge(industries_df, on=\"industry_id\", how=\"left\")\n",
    "\n",
    "# Keep only job_ids with valid industry_id matches\n",
    "clean_df = merged_df[merged_df[\"industry_name\"].notna()]\n",
    "\n",
    "# Save the cleaned data\n",
    "clean_df.to_csv(\"../data/transformed/jobs/it_job_industries_cleaned.csv\", index=False)\n",
    "print(clean_df[[\"job_id\", \"industry_id\", \"industry_name\"]].head())\n",
    "\n",
    "# Delete the old unclean file\n",
    "old_path = \"../data/transformed/jobs/it_job_industries.csv\"\n",
    "if os.path.exists(old_path):\n",
    "    os.remove(old_path)\n",
    "    print(f\"Deleted old file: {old_path}\")\n",
    "else:\n",
    "    print(f\"File not found, nothing to delete: {old_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02030d7",
   "metadata": {},
   "source": [
    "#### 7. Cleaning `postings.csv`\n",
    "Filtering by Job ID, Removing Irrelevant Columns, and Parsing Timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8088b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         job_id  company_name                         title  max_salary  \\\n",
      "22    133130219           NaN             Software Engineer         0.0   \n",
      "51   1014822088       Tracker          Marketing Specialist     80000.0   \n",
      "99   2974397965  Lynx Systems         Marketing Coordinator     85000.0   \n",
      "108  3169712432       SysMind  Salesforce Vlocity Developer         0.0   \n",
      "116  3245063922      Saxon AI                Data Architect         0.0   \n",
      "\n",
      "    pay_period                       location  company_id  views  med_salary  \\\n",
      "22         NaN  Los Angeles Metropolitan Area         0.0    1.0         0.0   \n",
      "51      YEARLY                  United States    255252.0    7.0         0.0   \n",
      "99      YEARLY                 Richardson, TX   6556800.0    3.0         0.0   \n",
      "108        NaN                Jersey City, NJ     85964.0  146.0         0.0   \n",
      "116        NaN              San Francisco, CA    224935.0    7.0         0.0   \n",
      "\n",
      "     min_salary  ... applies  remote_allowed  formatted_experience_level  \\\n",
      "22          0.0  ...     0.0             0.0                        <NA>   \n",
      "51      70000.0  ...     2.0             1.0                        <NA>   \n",
      "99      75000.0  ...     0.0             0.0                        <NA>   \n",
      "108         0.0  ...    16.0             0.0                        <NA>   \n",
      "116         0.0  ...     1.0             0.0                        <NA>   \n",
      "\n",
      "    skills_desc  work_type currency compensation_type normalized_salary  \\\n",
      "22          NaN  FULL_TIME      NaN               NaN               0.0   \n",
      "51          NaN  FULL_TIME      USD       BASE_SALARY           75000.0   \n",
      "99          NaN  FULL_TIME      USD       BASE_SALARY           80000.0   \n",
      "108         NaN   CONTRACT      NaN               NaN               0.0   \n",
      "116         NaN   CONTRACT      NaN               NaN               0.0   \n",
      "\n",
      "     listed_date listed_time_only  \n",
      "22    2024-04-19         13:58:45  \n",
      "51    2024-04-18         19:37:42  \n",
      "99    2024-04-18         20:19:57  \n",
      "108   2024-04-15         20:02:24  \n",
      "116   2024-04-19         14:27:02  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned IT jobs (only their job_ids)\n",
    "it_jobs = pd.read_csv(\"../data/transformed/jobs/it_job_industries_cleaned.csv\")\n",
    "it_job_ids = it_jobs['job_id'].unique()\n",
    "\n",
    "# Load the raw postings file\n",
    "df = pd.read_csv(\"../data/raw/postings.csv\")\n",
    "\n",
    "# Filter postings to keep only IT job_ids\n",
    "df = df[df['job_id'].isin(it_job_ids)]\n",
    "\n",
    "# Handle 'formatted_experience_level' as a string with NaNs for blanks\n",
    "if 'formatted_experience_level' in df.columns:\n",
    "    df['formatted_experience_level'] = df['formatted_experience_level'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "    df['formatted_experience_level'] = df['formatted_experience_level'].astype('string')\n",
    "\n",
    "# Drop fully empty (NaN) columns\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# Drop columns where all values are zero\n",
    "df = df.loc[:, (df != 0).any(axis=0)]\n",
    "\n",
    "# Drop irrelevant or redundant columns if they exist\n",
    "cols_to_drop = [\n",
    "    'job_posting_url', 'application_url', 'application_type',\n",
    "    'expiry', 'zip_code', 'fips', 'description', 'closed_time',\n",
    "    'posting_domain'\n",
    "]\n",
    "df = df.drop(columns=[col for col in cols_to_drop if col in df.columns], errors='ignore')\n",
    "\n",
    "# Replace empty strings with NaN\n",
    "df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "# Fill NaNs only in numeric columns with 0\n",
    "df.loc[:, df.select_dtypes(include='number').columns] = df.select_dtypes(include='number').fillna(0)\n",
    "\n",
    "# Convert 'listed_time' to datetime and split into date and time\n",
    "if 'listed_time' in df.columns:\n",
    "    df['listed_time'] = pd.to_datetime(df['listed_time'], unit='ms', errors='coerce')\n",
    "    df['listed_date'] = df['listed_time'].dt.date\n",
    "    df['listed_time_only'] = df['listed_time'].dt.time\n",
    "    df = df.drop(columns='listed_time')\n",
    "\n",
    "# Drop original columns\n",
    "df = df.drop(columns=['listed_datetime', 'original_listed_time'], errors='ignore')\n",
    "\n",
    "\n",
    "# Preview the cleaned DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_path = \"../data/transformed/postings\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Save cleaned DataFrame\n",
    "df.to_csv(f\"{output_path}/postings_it_cleaned.csv\", index=False, na_rep=\"NaN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6710adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_id                        0\n",
      "company_name                  0\n",
      "title                         0\n",
      "max_salary                    0\n",
      "pay_period                    0\n",
      "location                      0\n",
      "company_id                    0\n",
      "views                         0\n",
      "med_salary                    0\n",
      "min_salary                    0\n",
      "formatted_work_type           0\n",
      "applies                       0\n",
      "remote_allowed                0\n",
      "formatted_experience_level    0\n",
      "skills_desc                   0\n",
      "work_type                     0\n",
      "currency                      0\n",
      "compensation_type             0\n",
      "normalized_salary             0\n",
      "listed_date                   0\n",
      "listed_time_only              0\n",
      "dtype: int64\n",
      "Empty DataFrame\n",
      "Columns: [job_id, company_name, title, max_salary, pay_period, location, company_id, views, med_salary, min_salary, formatted_work_type, applies, remote_allowed, formatted_experience_level, skills_desc, work_type, currency, compensation_type, normalized_salary, listed_date, listed_time_only]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 21 columns]\n",
      "job_id                                 int64\n",
      "company_name                          object\n",
      "title                                 object\n",
      "max_salary                           float64\n",
      "pay_period                            object\n",
      "location                              object\n",
      "company_id                           float64\n",
      "views                                float64\n",
      "med_salary                           float64\n",
      "min_salary                           float64\n",
      "formatted_work_type                   object\n",
      "applies                              float64\n",
      "remote_allowed                       float64\n",
      "formatted_experience_level    string[python]\n",
      "skills_desc                           object\n",
      "work_type                             object\n",
      "currency                              object\n",
      "compensation_type                     object\n",
      "normalized_salary                    float64\n",
      "listed_date                           object\n",
      "listed_time_only                      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 1. See how many NaNs are in each column\n",
    "print(df.isna().sum())\n",
    "\n",
    "# 2. Inspect rows that have any NaNs\n",
    "print(df[df.isna().any(axis=1)])\n",
    "\n",
    "# 3. Check the types — some \"NaNs\" may be strings\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10f0b0b",
   "metadata": {},
   "source": [
    "#### 8. Loop Through Transformed CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4fe62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ../data/transformed\\companies\\it_companies.csv\n",
      "Processing: ../data/transformed\\companies\\it_company_industries.csv\n",
      "Processing: ../data/transformed\\companies\\it_employee_counts.csv\n",
      "Processing: ../data/transformed\\companies\\it_specialities.csv\n",
      "Processing: ../data/transformed\\jobs\\it_benefits.csv\n",
      "Processing: ../data/transformed\\jobs\\it_job_industries_cleaned.csv\n",
      "Processing: ../data/transformed\\jobs\\it_job_skills.csv\n",
      "Processing: ../data/transformed\\jobs\\it_salaries.csv\n",
      "Processing: ../data/transformed\\mappings\\industries_it_only.csv\n",
      "Processing: ../data/transformed\\postings\\postings_it_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transformed_dir = \"../data/transformed\"\n",
    "\n",
    "# Traverse all CSV files in all subfolders\n",
    "for root, dirs, files in os.walk(transformed_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            print(f\"Processing: {file_path}\")\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "\n",
    "                # Replace empty strings with NaN across all columns\n",
    "                df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "                # Fill numeric columns with 0\n",
    "                num_cols = df.select_dtypes(include=['number']).columns\n",
    "                df[num_cols] = df[num_cols].fillna(0)\n",
    "\n",
    "                # Ensure string columns remain as NaN where missing\n",
    "                str_cols = df.select_dtypes(include=['object', 'string']).columns\n",
    "                df[str_cols] = df[str_cols].astype('string')\n",
    "\n",
    "                # Save back to the same file\n",
    "                df.to_csv(file_path, index=False)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {file_path}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
